{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../asset/LOGO_TSP.jpg\" height=\"100\">\n",
    "<img src=\"../asset/LOGO_MBDA.png\" height=\"100\">\n",
    "\n",
    "## YOLOv5 Implementation on Phytec IMX8+ NPU *(ie. Neural Processing Unit)*\n",
    "##### Author : *Iwan Ibnoulouafi*\n",
    "\n",
    "This is our custom made Notebook which encompasses multiple tools allowing to exploit a choosen **YOLOv5** model.\n",
    "\n",
    "---\n",
    "\n",
    "#### List of the **available tools** : \n",
    "* **Training** a model on a specified dataset *(uses `train.py`)*\n",
    "* **Validate** a model on a specified validation dataset *(uses `val.py`)*\n",
    "* **Detection** to run inference of model on a specified input *(uses `detect.py`)*\n",
    "* **Prune** a model to a given sparsity and run a validation on the pruned model *(uses `val_prune.py`)*\n",
    "* **Quantize** a model to `uint8`\n",
    "\n",
    "> Most of them are directly forked from [Ultralytics YOLOv5 repo](https://github.com/ultralytics/yolov5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "\n",
    "Install the dependencies (provided by `requirements.txt`) and check PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commment if already done\n",
    "#Use pip instead of pip3 if you're running Python 2.X.X or earlier\n",
    "!pip3 install -r requirements.txt\n",
    "import torch\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Detect\n",
    "\n",
    "`detect.py` runs YOLOv5 inference on a variety of sources and saving results to `runs/detect`. Example inference sources are:\n",
    "\n",
    "```shell\n",
    "python detect.py --source 0  # webcam\n",
    "                          img.jpg  # image\n",
    "                          vid.mp4  # video\n",
    "                          screen  # screenshot\n",
    "                          path/  # directory\n",
    "                         'path/*.jpg'  # glob\n",
    "                         'https://youtu.be/LNwODJXcvt4'  # YouTube\n",
    "                         'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "```\n",
    "\n",
    "#### Flags description : \n",
    "\n",
    "* `--weights` : Path to your model\n",
    "\n",
    "* `--img` : Inference image input size \n",
    "\n",
    "* `--conf` : Objects detected with confidence above this value will be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: python\n"
     ]
    }
   ],
   "source": [
    "!python3 detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Validate\n",
    "`val.py` validates a model's accuracy on a given dataset. \n",
    "\n",
    "#### Flags description : \n",
    "\n",
    "* `--data` : Path to the dataset `.yaml` file.\n",
    "\n",
    "* `--verbose` : Displays results by class\n",
    "\n",
    "* All other flags are the same as `detect.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 val.py --weights yolov5s.pt --data coco128.yaml --img 640 --half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train\n",
    "Train a YOLOv5 model on a given dataset, starting from a pretrained `--weights path_to_model`, or from randomly initialized `--weights '' --cfg path_to_model_yaml`.\n",
    "\n",
    "**Training result** are saved to `runs/train/`.\n",
    "\n",
    "#### Flags description :\n",
    "\n",
    "* `--epoch`: number of epochs for the model training\n",
    "\n",
    "* `--batch`: defines the batch size\n",
    "\n",
    "* `--device`: device on which to train (ie. `cuda` for gpu, `mps` for Apple Silicon)\n",
    "\n",
    "* All other flags are the same as `detect.py` & `val.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prune\n",
    "Prune a pretrained YOLOv5 model to a given sparsity. The pruning is applied using `utils.torch_utils.prune` which implements **unstructured pruning**.\n",
    "\n",
    "`val_prune.py` first **prune**, **export** then **validate** the pruned model on a given dataset.\n",
    "\n",
    "#### Flags description :\n",
    "\n",
    "* `--sparsity`: percentage of the model that will be set to zero\n",
    "\n",
    "* All other flags are same as `detect.py`, `val.py` & `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 val_prune.py --weights yolov5s.pt --data coco128.yaml --img 640 --sparsity 0.3 --device mps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Quantize\n",
    "Quantize a given YOLOv5 model to `uint8` for compatibility with NPU. This code is heavily inspired by **Phytec IMX-8+ Software Documentation**.\n",
    "\n",
    "This code first export the model to **TF saved_model format** and the quantize it in a **TFLite format** using **PTQ (Post-Training Quantization)**\n",
    "\n",
    "For calibration of the model during the quantization, a **calibration dataset** is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use to quantize a pruned model\n",
    "!python3 export_for_prune.py --weights yolov5s_prune_0.3.pt --include saved_model --device mps\n",
    "\n",
    "#Use to quantize other models\n",
    "#!python3 prune.py --weights yolov5s.pt --include saved_model --device mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "#Path to the calibration dataset image's folder\n",
    "dataset_dir = \"../datasets/mycoco100/images/val2017\"\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    img_file = [os.path.join(dataset_dir, i) for i in os.listdir(dataset_dir) if i.endswith('.jpg')]\n",
    "\n",
    "    for img_path in img_file:\n",
    "        img = cv.imread(img_path)\n",
    "        # Resize image to the input size expected by the model (e.g., 640x640 for YOLOv5)\n",
    "        img_resized = cv.resize(img, (640, 640))\n",
    "        # Normalize image to range [0, 1] if required\n",
    "        img_normalized = img_resized / 255.0\n",
    "        # Convert to float32 if the model expects it\n",
    "        img_input = np.expand_dims(img_normalized, axis=0).astype(np.float32)\n",
    "        # Yield the preprocessed image\n",
    "        yield [img_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"yolov5s_prune_0.3_saved_model\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "\n",
    "#Define the input image type of the quantized model\n",
    "converter.inference_input_type = tf.float32 \n",
    "\n",
    "#Define the output image type of the quantized model\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "open('yolov5s_quant.tflite', 'wb').write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Export (and Quantize)\n",
    "\n",
    "Export a given YOLOv5 model from a `.pt` format to a given format *(see `export.py` for available formats)*\n",
    "\n",
    "#### Flags description :\n",
    "\n",
    "* `--int8`: Apply `int8` quantization for Tensorflows model.\n",
    "\n",
    "* `--imgsz`: Correspond to `--img` in the other scripts\n",
    "\n",
    "* All other flags are the same as `detect.py`, `val.py` & `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use to export a pruned model\n",
    "#!python3 export_for_prune.py --weights yolov5s_prune_0.3.pt --include onx\n",
    "\n",
    "#Use to export other models\n",
    "!python3 export.py --weights yolov5s.pt --include onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
